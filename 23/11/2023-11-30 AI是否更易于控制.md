---
title: 2023-11-30 AI是否更易于控制
tags: 
grammar_cjkRuby: true
---



https://optimists.ai/2023/11/28/ai-is-easy-to-control/
为什么今年要投入数十亿美元用于人工智能研发？公司当然希望获得投资回报。可以说，人工智能有利可图的主要原因是它比人类劳动力更可控。人工智能的个性和行为可以比任何人类员工更精细地控制。像 GPT-4 和 Claude 这样的聊天机器人会经历一个“监督微调”阶段，其中它们的神经电路被直接优化以在指定的上下文中说出某些单词、短语和句子。直接偏好优化（DPO）和人类反馈强化学习（RLHF）等算法也被用来将聊天机器人的大脑整体塑造成人类评分者最喜欢的系统。通过仔细整理人工智能最初训练的数据集，我们还可以控制人工智能所有最具影响力的经历。由于人工智能是计算机程序，因此可以廉价地复制它们并在许多计算机上并行运行，这使得投资数百万甚至数十亿美元仔细培训“单个”人工员工在经济上是可行的。不用说，这一切对人类员工来说都是不可能或不道德的。1

如今，许多人担心我们会失去对人工智能的控制，导致人类灭绝或类似灾难性的“人工智能接管”。我们希望本文中的论点使这样的结果看起来难以置信。但即使未来的人工智能在严格意义上变得不那么“可控”——例如，仅仅因为它的思考速度比人类直接监督的速度快——我们也认为，将我们的价值观灌输给人工智能是很容易的，这个过程称为“对齐”。按照设计，一致的人工智能将优先考虑人类的安全和福利，为人类的积极未来做出贡献，即使在它们获得当今人类拥有的自主水平的情况下也是如此。

接下来，我们将认为，在可预见的未来，人工智能，甚至是超人的人工智能，仍将比人类更加可控。由于每一代可控人工智能都可以帮助控制下一代，因此这个过程似乎可以无限期地持续下去，甚至达到非常高的能力水平。因此，我们认为灾难性 的人工智能接管的可能性约为 1% ——尾部风险2值得考虑，但不是世界上主要的风险来源。我们不会尝试在本文中直接解决悲观论点，尽管我们将在即将发布的文件中这样做。相反，我们的目标是提出对人类控制和调整人工智能进入遥远未来的能力持乐观态度的基本理由。

# AI 是白盒
我们经常解决的许多“协调问题”，比如抚养孩子或训练宠物，似乎比训练一个友好的人工智能要困难得多。造成这种情况的一个主要原因是人类和动物的大脑是黑匣子，从某种意义上说，我们实际上无法观察它们内部发生的所有认知活动。我们不知道哪些神经元何时放电，我们没有神经元之间的连接图，也不知道每个突触的连接强度。我们用于非侵入性测量大脑的工具（例如脑电图和功能磁共振成像）仅限于神经元放电的非常粗粒度的相关性，例如电活动和血流。可以侵入性地插入电极来测量单个神经元，但这些电极仅覆盖全部 860 亿个神经元和 100 万亿个突触中的一小部分。

## 黑盒方法足以进行人类对齐
如果我们能够观察并修改人脑中发生的一切，我们就能够使用优化算法来计算对突触权重的精确修改，这将导致期望的行为变化。由于我们做不到这一点，我们被迫求助于粗糙且容易出错的工具来将年轻人塑造成善良和富有成效的成年人。我们为孩子们提供可供模仿的榜样，并根据他们与生俱来、进化的动力量身定制奖励和惩罚。本质上，我们是从外部刺探人脑的学习算法，而不是直接设计这些学习算法。

这些黑匣子调整方法的效果令人震惊：大多数人确实很好地吸收了他们的文化价值观，而且大多数人都相当亲社会。但人类的排列也非常不完美。很多人在可以逃脱惩罚的情况下都是自私和反社会的，而且文化规范确实会随着时间的推移而改变，无论好坏。黑盒对齐是不可靠的，因为不能保证旨在改变某个方向的行为的干预实际上会改变该方向的行为。孩子们经常做与父母告诉他们的事情完全相反的事情，只是为了叛逆。

## 目前的AI对齐方法都是白盒
相比之下，使用人工神经网络（ANN）实现的人工智能是白盒，因为我们对其内部拥有完全的读写访问权限。它们只是一种特殊类型的计算机程序，我们可以随心所欲地分析和操作计算机程序，而且基本上不需要任何成本。这使得许多强大的对齐方法成为可能，而这些方法对于大脑来说是不可能的。反向传播算法（“backprop”） 是一个重要的例子。

反向传播有效地计算最佳方向（称为“梯度”），在该方向上改变 ANN 的突触权重，以便根据我们指定的任何标准最大限度地提高其性能。下面的动画显示了反向传播用于计算对数字 5 的图像进行分类的 ANN 的梯度。通过运行反向传播、沿着梯度将权重微移一小步来训练 ANN，然后再次运行反向传播，依此类推多次，直到性能停止增加。不用说，我们无法对人脑或任何其他动物的大脑进行像梯度下降这样的远程操作！


梯度下降非常强大，因为与黑盒方法不同，它几乎不可能被欺骗。人工智能的所有想法对于梯度下降都是“透明”的，并且包含在其计算中。如果人工智能秘密计划杀死你，梯度下降会注意到这一点，并使其将来不太可能这样做，因为制作秘密谋杀情节所需的神经电路可以被拆除并重新配置为直接提高性能的电路。一般来说，梯度下降有一种强烈的倾向，偏向于表现良好的最简单的解决方案，而秘密谋杀阴谋对于提高人类实际优化人工智能执行的任务的性能并没有积极的作用。

# 认知干预
与永久改变整体行为的梯度下降不同，认知干预是对系统大脑活动的临时修改，以实时影响其行为。我们很少使用这些技术来控制其他人，而当我们这样做时，干预措施总是粗鲁和不精确的，例如让某人喝醉以使他们更愉快。部分原因是我们只能通过黑匣子访问大脑，但也因为人类拥有法律保护，可以免受大多数形式的影响。

然而，此类 技术 在人工智能控制研究中相当 常见 。人工智能没有任何权利，因此可以使用开发人员可以想象的任何技术来控制它们。我们可以进行大量的实验来找到最有效的干预措施，我们还可以在各种模拟环境中运行它，并测试它在有或没有认知干预的情况下是否表现得符合预期。每次人工智能的“记忆”都可以重置，使实验可以完美重现，并阻止人工智能适应我们的行为，这与心理学和社会科学中的实验非常不同。

# 感官干预
由于人工智能是白盒，我们可以完全控制它们的“感官环境”（无论是由文本、图像还是其他形式组成）。人工智能有一个完整的子领域，称为提示工程，致力于搜索使语言模型与人类偏好保持一致的文本输入（“提示”），并且即使没有梯度下降，这些提示也可以在孤立的情况下非常有效。由于我们可以完美地重置人工智能的状态和记忆，我们可以运行无限数量的受控实验，看看哪种类型的提示最能有效地诱导所需的行为，而人工智能却没有办法适应。相比之下，营销和宣传可以说是“人类即时工程”的例子，但由于进行受控和个性化实验的困难，即使是最好的宣传者也只能以非常分散和不可靠的方式控制人们。

事实上，提示是控制人工智能的一种有效方法，它常常可以用来让人工智能采取违背其创造者意图的行动。例如，细心的用户可以设计称为“越狱”的提示，导致 GPT-4 输出与 OpenAI 内容政策相矛盾的文本。这导致了模型创建者和用户之间的打地鼠游戏，创建者修补他们的模型以抵抗当前的越狱，而用户则想出新的越狱。

一些人指出越狱的有效性是人工智能难以控制的一个论点。我们认为这种说法根本没有道理，因为越狱本身就是一种AI控制方式。绝大多数越狱事件的发生是因为人类用户希望人工智能做一些与其创造者意图相反的事情，并使用基于提示的控制方法让人工智能按照用户想要的方式行事。大多数越狱都是人工智能被成功控制的例子，只是由不同的人通过不同的方法控制。GPT-4 和 Claude 在正常对话中并没有试图越狱。

粗略的白盒对齐在自然界中效果很好
几乎所有有大脑的生物体都有一个与生俱来的奖励系统。随着有机体的学习和成长，其奖励系统会直接更新其神经回路，以强化某些行为并惩罚其他行为。由于奖励系统使用简单的学习规则直接有针对性地更新它，因此它可以被视为白盒对齐的粗略形式。这一生物学证据表明，白盒方法是塑造智能系统内在动机的非常强大的工具。我们的奖励回路可靠地在每个人的心理中留下了一组动机不变量：我们对朋友和熟人有同理心，我们有为人父母的本能，当别人伤害我们时我们想要报复等等。此外，这些不变量必须通过简单的方法产生。欺骗奖励信号，这些信号足够简单，可以在基因组中编码。

这表明至少可以使用类似简单的奖励函数来调整人类水平的通用人工智能。但我们已经将尖端模型与学习的奖励函数结合起来，这些函数过于复杂，无法适应人类基因组，因此在这个问题上，我们可能比我们自己的奖励系统领先一步。至关重要的是，这并不意味着人类“与进化论保持一致”——参见《进化论》没有提供任何证据证明昆汀·波普的左转可以揭穿这一类比。相反，我们与我们的奖励系统在我们的环境中可预测地产生的价值观保持一致。

一位研究 10 万年前人类的人类学家不会说人类符合进化论，也不会说人类符合生育尽可能多的婴儿的观点。他们会说我们有一些相当普遍的倾向，比如同理心、养育本能和报复。他们可能预测这些价值观将随着时间和文化变迁而持续存在，因为它们是由根深蒂固的生物奖励系统产生的。他们是对的。

对于人工智能来说，我们就是天生的奖励系统。不难预测我们的奖励信号将产生什么值：它们是明显的值，人类学家或心理学家会说人工智能似乎在训练期间显示的值。有关更多讨论，请参阅人类提供了有关对齐的未开发的丰富证据。

# AI控制研究更容易
目前人工智能不仅比人类更可控，而且提高人工智能可控性的研究比提高人类可控性的研究要容易得多，所以我们应该期望人工智能比人类更快地变得更可控。以下是一些原因：

重现性：每次实验结束后，您始终可以将 AI 恢复到准确的原始状态，然后运行不同的实验，并确保两个实验之间不会发生干扰。这使得隔离关键变量变得更加容易，并允许获得更多可重复的结果。
成本：人工智能是更便宜的研究对象。即使是最大的模型，例如 GPT-4，其成本也只是实际人类受试者的一小部分。这使得研究变得更容易，从而更快。
可扩展性：人工智能是更便宜的干预目标。用于控制人工智能的干预可以轻松扩展到目标人工智能的许多副本。一个耗资 5000 万美元的流程如果要让一个人得到完美控制，那可不是很经济。然而，花费 5000 万美元来生产完美控制的人工智能绝对是值得的。这使得人工智能研究人员能够更加雄心勃勃地实现他们的研究目标。
法律地位：人工智能受到研究人员的保护要少得多。人类主体拥有“权利”和“法律保护”，并且能够提起“诉讼”。欺骗、操纵、威胁或以其他方式残酷对待人工智能不会产生任何后果。因此，控制研究可以探索各种可能的谎言、威胁、贿赂、情感勒索以及其他在人类身上尝试有风险的伎俩。
社会认可：控制人工智能是比控制人类更道德的目标。如果你说你正在研究更好地控制人类的方法，人们会奇怪地看着你。结果，人类控制研究人员不得不用“营销策略师”或“政治顾问”等委婉说法来称呼自己，并且必须从尴尬的角度和有限的工具来解决人类控制问题的核心。
所有这些原因都表明人工智能控制研究的进展将比人类控制研究快得多。目前人工智能可控性领先于人类可控性。人工智能可控性的极限远远超出了人类可控性的极限。因此，我们应该预期未来的人工智能也将比人类更可控，而且差距会随着时间的推移而越来越大。

# 价值观很容易学习
即使在人工智能不再遵守我们的每一个命令的悲观场景中，它们仍然会保护我们并改善我们的福利，因为它们很早就在训练中学习了道德准则。

目前法学硕士的道德判断已经 在很大程度上符合常识，而法学硕士在遇到道德模糊的场景时通常会表现出适当程度的不确定性。这强烈表明，当人工智能正在接受训练时，它会在获得自我意识、自主复制自身的能力或开发新技术的能力等危险能力之前，对人类价值观有相当深入的理解。

这意味着人工智能不会学会“玩训练游戏”，在训练中假装道德，但暗地里却别有用心。如果人工智能首先学习道德，它会希望帮助我们确保它在变得更强大时保持道德。价值观很容易学习，主要原因有两个：

值在语言模型预训练数据集中普遍存在。本质上，每个话语领域都包含隐含或明确的评价判断。相比之下，危险能力所需的知识类型在训练语料库中出现的频率要低得多。
由于价值观几乎为社会中的每个人所共享和理解，因此它们不可能非常复杂。与科学和技术不同的是，科学和技术的分工能够积累更加复杂的知识，而价值观必须保持足够简单，以便儿童在几年内就能学会。

因为价值观很简单，所以当前的语言模型已经非常有能力从道德上评估超级智能可能具有的复杂行为。在上面的非樱桃选择的例子中，GPT-4 表明它可以认识到杀人是错误的，即使是在它不知道如何发明的未来技术的“分布外”场景中也是如此。通过说“使用任何技术来伤害个人” ， GPT 表明它已经理解了“不伤害”这一基本的简单道德规则，使其能够将其道德规范推广到远远超出培训范围的案例。引人注目的是，GPT-4 还能够从上下文中推断出术语“unspool”正在以一种特殊的方式使用来指代某种伤害。 

除了所示的示例场景之外，GPT-4 在讨论超出其训练分布的强大功能的使用时通常持谨慎态度，即使是为了无害的目的。它经常建议“咨询专家”、“安全考虑”、“考虑道德影响和潜在风险”、“遵守法律法规”等。即使讨论的功能远远超出了培训中直接解决的功能，它也会很快建议不要采取看似可能影响人类的行为。这些例子强烈表明，一致性比能力更具有普遍性。我们应该预期，我们在短期内灌输给人工智能的价值观将得到很好的保留，即使它们在许多任务上超越了人类的表现。

# 结论
有很多理由可以预期人工智能将易于控制并且易于与人类价值观保持一致。与人类认知的“黑匣子”相比，人工智能的“白匣子”性质可以实现精确有效的优化和控制方法，而这在有机大脑上是不可能使用的。这些方法，再加上训练数据集中人类价值观固有的简单性和普遍性，让我们确信人工智能能够并且将会深刻地内化这些价值观。我们现在对人工智能的控制不仅很强，而且将逐渐超过我们控制人类行为的能力。人工智能控制研究由于其可重复性、成本效益、可扩展性以及缺乏法律和道德约束而正在迅速发展。我们预计，随着人工智能能力的提高，它们也将与我们的价值观保持一致，确保与这些先进系统安全、有益地共存。

在这篇文章中，我们直接将人工智能的可控性与人类的可控性进行比较，这种比较方式可能会让一些读者感到反感或不尊重。我们强烈谴责像控制人工智能一样控制人类的企图，我们认为现在开始讨论人工智能控制的道德规范很重要。未来的人工智能将以值得严肃伦理考虑的方式表现出情感和欲望。↩︎
使用更宽松的SKEW 指数将“尾部风险”定义为偏离平均值两个标准差以上的事件。其他来源将其定义为三个标准。与平均值的偏差。