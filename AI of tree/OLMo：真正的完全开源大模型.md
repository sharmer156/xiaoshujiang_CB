---
title: OLMo：真正的完全开源大模型
tags: 
grammar_cjkRuby: true
---


OLMo：真正的完全开源大模型

OLMo（Open Language Model）与其他开源语言模型的不同之处在于其“完全开放的框架”。

OLMo不仅100%开放了其完整的预训练数据——3万亿token的Dolma数据集

还提供了其训练代码、模型权重、推理代码、训练指标和完整日志等全部原始数据。

真正做到完全开源，100%开源！

这种程度的开放使研究人员能够完全复现模型训练过程，理解模型的性能表现，以及根据需要对模型进行微调。

OLMo框架包括：

1、完整的预训练数据：OLMo项目提供了完整的预训练数据——AI2的Dolma数据集，这是一个包含三万亿token的开放语料库，用于语言模型的预训练。这意味着研究人员不仅可以访问模型本身，还能够获得用于训练这些模型的原始数据，从而允许他们深入理解模型的学习基础，甚至重新训练或调整模型以适应特定的研究需求。

2、训练代码和模型权重：OLMo框架提供了四种不同变体模型的完整模型权重，每种模型都至少训练到2万亿令牌。除了数据，OLMo还提供了训练代码、模型权重、推理代码、训练指标和日志。这种程度的开放性使研究人员能够完全复现模型训练过程，理解模型的性能表现，以及根据需要对模型进行微调。

3、评估工具的提供：项目包含了开发过程中使用的评估套件，以及500多个模型的检查点，每1000步训练过程中的每一个都有，还有评估代码，这些都属于Catwalk项目的一部分。这使得研究人员可以使用相同的工具来评估自己的模型或对OLMo模型进行进一步的分析。

模型参数和架构

OLMo提供了不同规模的模型变体，具体包括：

- 1B（10亿参数）模型：具有16层，每层2048个隐藏单元，16个注意力头，训练了至少2万亿个令牌。

- 7B（70亿参数）模型：包含32层，每层有4086个隐藏单元，32个注意力头，训练了约2.46万亿个令牌。

- 65B（650亿参数）模型：（文章撰写时仍在训练中），计划包含80层，每层8192个隐藏单元，64个注意力头。

这些模型采用了基于Vaswani等（2017年）的解码器仅Transformer架构，并进行了多项改进，例如：

- 不使用偏置项，以提高训练稳定性。
- 采用非参数层归一化。
- 使用SwiGLU激活函数代替ReLU。
- 引入旋转位置嵌入（RoPE）。
- 使用修改版的BPE-based标记器，以减少个人可识别信息（PII）。

预训练数据：Dolma

OLMo使用的Dolma数据集是一个多源、多样性的3万亿令牌语料库，涵盖了从7种不同数据源获取的5亿文档。这个数据集旨在促进语言模型预训练的开放研究，并包括网络页面、代码、社交媒体、STEM论文、书籍和百科资料等内容。

性能评估：

OLMo 7B在许多生成和阅读理解任务（如truthfulQA）上与Llama 2不相上下，但在流行的问答任务（如MMLU或Big-bench Hard）上略微落后。

使用AI2的Paloma和可用检查点，分析了模型预测语言的能力与模型规模因素（如训练令牌数）之间的关系。Paloma试图通过平等地采样每个领域来更均衡地代表LLM使用的许多领域。

项目地址：https://allenai.org/olmo

模型下载：https://huggingface.co/allenai/OLMo-7B

技术报告：https://blog.allenai.org/olmo-open-language-model-87ccfc95f580

论文：https://arxiv.org/abs/2402.00838

GitHub：https://github.com/allenai/olmo