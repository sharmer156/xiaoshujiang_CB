---
title: 2024-6-21 94 行代码就是训练神经网络所需的一切
tags: 
grammar_cjkRuby: true
---


These 94 lines of code are everything that is needed to train a neural network. Everything else is just efficiency.

This is my earlier project Micrograd. It implements a scalar-valued auto-grad engine. You start with some numbers at the leafs (usually the input data and the neural network parameters), build up a computational graph with operations like + and * that mix them, and the graph ends with a single value at the very end (the loss). You then go backwards through the graph applying chain rule at each node to calculate the gradients. The gradients tell you how to nudge your parameters to decrease the loss (and hence improve your network).

Sometimes when things get too complicated, I come back to this code and just breathe a little. But ok ok you also do have to know what the computational graph should be (e.g. MLP -> Transformer), what the loss function should be (e.g. autoregressive/diffusion), how to best use the gradients for a parameter update (e.g. SGD -> AdamW) etc etc. But it is the core of what is mostly happening.

The 1986 paper from Rumelhart, Hinton, Williams that popularized and used this algorithm (backpropagation) for training neural nets:
https://cs.toronto.edu/~hinton/absps/naturebp.pdf
micrograd on Github: https://github.com/karpathy/micrograd
and my (now somewhat old) YouTube video where I very slowly build and explain:
https://youtube.com/watch?v=VMj-3S1tku0
这 94 行代码就是训练神经网络所需的一切。其他一切都只是效率。

这是我早期的项目 Micrograd。它实现了标量值自动分级引擎。您从叶子上的一些数字（通常是输入数据和神经网络参数）开始，使用 + 和 * 等混合它们的操作构建一个计算图，并且该图在最后以单个值结束（损失）。然后，您可以向后浏览图表，在每个节点应用链式法则来计算梯度。梯度告诉您如何调整参数以减少损失（从而改善您的网络）。

有时，当事情变得太复杂时，我会回到这段代码并稍稍喘口气。但是好吧，你还必须知道计算图应该是什么（例如 MLP -> Transformer）、损失函数应该是什么（例如自回归/扩散）、如何最好地使用梯度进行参数更新（例如 SGD - > AdamW) 等等等等。但它是大多数发生的事情的核心。

Rumelhart、Hinton、Williams 于 1986 年发表的论文推广并使用了这种算法（反向传播）来训练神经网络：
 https://cs.toronto.edu/~hinton/absps/naturebp.pdf 
Github 上的 micrograd： https://github.com/karpathy/micrograd 
以及我的（现在有点旧的）YouTube 视频，我在其中非常缓慢地构建和解释：
 https://youtube.com/watch?v=VMj-3S1tku0 
 ![enter description here](https://i.imgur.com/SKQ50H8.png)
 
 Evaluating the Openness of Open Source AI Models

Many AI models claim to be open but restrict code & data access.

Companies like Meta & Microsoft label their models open but share little info. This practice, called open-washing, fakes transparency.

Truly open models should let researchers replicate and examine them, which isn't always true.

#open #ai #genai #llm #meta #microsoft #chatglm #mistral #qwen #mixtral #falcon #commandr #gemma #llama #chatgpt
评估开源人工智能模型的开放性

许多人工智能模型声称是开放的，但限制代码和数据访问。

Meta 和 Microsoft 等公司将其模型标记为开放，但共享的信息很少。这种被称为公开清洗的做法伪造了透明度。

真正的开放模型应该让研究人员复制和检查它们，但这并不总是正确的。

 #open #ai #genai #llm #meta #microsoft #chatglm #mistral #qwen #mixtral #falcon #commandr #gemma #llama #chatgpt 
 ![enter description here](https://i.imgur.com/IMhXz1b.jpeg)